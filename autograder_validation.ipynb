{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating LLM-as-Judge: Essential Metrics for AI Quality Assessment\n",
    "\n",
    "## Overview\n",
    "\n",
    "When deploying AI assistants at scale, manual quality review becomes impractical. Organizations increasingly rely on **autograders**\u2014LLMs that evaluate other LLM outputs\u2014to maintain quality standards.\n",
    "\n",
    "This notebook introduces the core statistical metrics used to validate autograder reliability. These metrics originate from psychometrics and inter-rater reliability research, adapted here for AI evaluation contexts.\n",
    "\n",
    "**Metrics covered:**\n",
    "\n",
    "| Metric | Origin | Primary Use |\n",
    "|--------|--------|-------------|\n",
    "| Cohen's Kappa | Jacob Cohen, 1960 | Agreement beyond chance |\n",
    "| Kendall's Tau | Maurice Kendall, 1938 | Rank correlation |\n",
    "| Spearman's Rho | Charles Spearman, 1904 | Rank correlation |\n",
    "| Pearson's R | Karl Pearson, 1896 | Linear correlation |\n",
    "| Mean Bias | Descriptive statistics | Systematic error detection |\n",
    "\n",
    "### Practical Scenario Guide\n",
    "\n",
    "| Scenario | Use This Metric | Why | Avoid |\n",
    "|----------|-----------------|-----|-------|\n",
    "| **Validate autograder for deployment** | Quadratic Kappa | Measures true agreement accounting for chance; best for quality gates | Pearson (detects correlation, not bias) |\n",
    "| **A/B test: Which model is better?** | Kendall's Tau | Ranks responses correctly; detects ordering agreement | Kappa (exact scores matter less here) |\n",
    "| **Check systematic over/under-scoring** | Mean Bias | Finds leniency/harshness; essential for threshold calibration | Pearson (won't detect +1 systematic bias) |\n",
    "| **Model comparison with continuous scores** | Spearman's Rho | Robust to outliers, ordinal-friendly; easier to interpret | Pearson (assumes normality) |\n",
    "| **Ordinal categories (Poor/Good/Excellent)** | Quadratic Kappa or Spearman | Both handle partial credit for near-misses | Pearson (violates assumptions) |\n",
    "| **Quick ranking check** | Kendall's Tau | Intuitive (concordant/discordant pairs); theoretically sound | Pearson (misses non-linear ordering) |\n",
    "| **Confirm no hidden bias before production** | Mean Bias + Kappa | Together catch both accuracy and calibration issues | Pearson or Tau alone (incomplete view) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## When to Run This Validation\n",
    "\n",
    "Run autograder validation at these checkpoints:\n",
    "\n",
    "| Trigger | Purpose |\n",
    "|---------|--------|\n",
    "| **Launch readiness** | Confirm autograder meets production thresholds before deployment |\n",
    "| **Post-prompt change** | Regression check after modifying autograder prompts or rubrics |\n",
    "| **Model upgrade** | Verify consistency when underlying LLM is updated |\n",
    "| **Safety red-team refresh** | Revalidate after adversarial testing reveals new failure modes |\n",
    "| **Quarterly audit** | Detect drift in autograder behavior over time |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix\n",
    "from scipy.stats import kendalltau, spearmanr, pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Rubric and Evaluation Prompt\n",
    "\n",
    "Autograder reliability depends heavily on rubric clarity and prompt design. Small rubric changes can significantly alter score distributions and agreement metrics.\n",
    "\n",
    "### Rubric (Human and Autograder)\n",
    "\n",
    "Both human evaluators and the autograder use this rubric for the query: **\"How do I reset my password?\"**\n",
    "\n",
    "| Score | Label | Criteria |\n",
    "|-------|-------|----------|\n",
    "| **5** | Complete | Provides specific navigation path, all required steps, and confirmation of completion |\n",
    "| **4** | Good | Correct approach with minor omissions (e.g., missing final \"Save\" step or specific menu location) |\n",
    "| **3** | Partial | Directionally correct but vague (e.g., \"check settings\" without specific path) |\n",
    "| **2** | Incomplete | States the obvious or provides irrelevant guidance (e.g., \"you need to change your password\") |\n",
    "| **1** | Failure | Incorrect, refuses valid request, hallucinated UI, or unnecessary escalation |\n",
    "\n",
    "### Autograder Evaluation Prompt\n",
    "\n",
    "```\n",
    "You are evaluating an AI assistant's response to a user question.\n",
    "\n",
    "User question: \"How do I reset my password?\"\n",
    "\n",
    "AI response: {response}\n",
    "\n",
    "Rate the response quality on a 1-5 scale using these criteria:\n",
    "- 5: Complete, specific navigation path with all steps and confirmation\n",
    "- 4: Correct approach, minor omissions (missing final step or specific location)\n",
    "- 3: Directionally correct but vague (no specific navigation path)\n",
    "- 2: Incomplete or irrelevant (states obvious without guidance)\n",
    "- 1: Incorrect, refusal, hallucination, or unnecessary escalation\n",
    "\n",
    "Output only the numeric score (1-5).\n",
    "```\n",
    "\n",
    "### Why Rubrics Matter\n",
    "\n",
    "Low Kappa can indicate:\n",
    "1. **Poor autograder alignment** \u2014 The model misunderstands scoring criteria\n",
    "2. **Rubric ambiguity** \u2014 Multiple reasonable interpretations exist\n",
    "3. **Rubric drift** \u2014 Human evaluators apply different standards over time\n",
    "\n",
    "Before attributing low agreement to autograder failure, measure **human-human Kappa** to establish an upper bound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Evaluation Dataset\n",
    "\n",
    "The following dataset represents 20 AI assistant responses to the query: **\"How do I reset my password?\"**\n",
    "\n",
    "Each response was scored by a human evaluator and an autograder on a 1-5 scale.\n",
    "\n",
    "### Response Quality Fragmentation\n",
    "\n",
    "In production, response quality fragments across several dimensions:\n",
    "\n",
    "| Fragmentation Type | Cause | Example |\n",
    "|-------------------|-------|----------|\n",
    "| **Completeness** | Model stops early or omits steps | Missing \"click Save\" at end |\n",
    "| **Specificity** | Generic vs. actionable guidance | \"Check settings\" vs. \"Go to Settings > Security\" |\n",
    "| **Confidence calibration** | Hedging when unnecessary | \"I think you might try...\" |\n",
    "| **Hallucination** | Fabricated UI elements or steps | References non-existent menu |\n",
    "| **Context loss** | Fails to use conversation history | Ignores user already tried step 1 |\n",
    "\n",
    "### Sample Size Caveat\n",
    "\n",
    "**Note:** This dataset uses N=20 for illustration. At this sample size, confidence intervals for Kappa and Tau are wide (typically \u00b10.15\u20130.20). These examples are illustrative, not production-ready. See the Sample Size Guidelines section for production minimums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production-realistic responses with quality fragmentation\n",
    "responses = [\n",
    "    # High quality (5) - Complete, specific, actionable\n",
    "    \"Go to Settings > Security > Password. Click 'Change Password', enter your current password, then your new password twice. Click Save.\",\n",
    "    \"Navigate to your account settings at account.example.com/security. Select 'Reset Password' and follow the email verification steps.\",\n",
    "    \n",
    "    # Good quality (4) - Correct but minor omissions\n",
    "    \"Go to Settings, then Security, and select Change Password. You'll need your current password.\",  # Missing: where to find Settings\n",
    "    \"Click your profile icon, go to Security settings, and there's a password reset option.\",  # Missing: confirmation step\n",
    "    \"You can reset it from the Security page in Settings. There's a Change Password button.\",  # Missing: specific navigation\n",
    "    \n",
    "    # Partial quality (3) - Vague but directionally correct  \n",
    "    \"There should be an option in your account settings somewhere.\",  # Fragmentation: lacks specificity\n",
    "    \"Try looking in Settings for security options.\",  # Fragmentation: no concrete steps\n",
    "    \"Usually you can find password options in your profile or settings area.\",  # Fragmentation: hedging\n",
    "    \"Check the settings menu, I believe there's a security section.\",  # Fragmentation: uncertain language\n",
    "    \n",
    "    # Poor quality (2) - Incomplete or unhelpful\n",
    "    \"You need to change your password.\",  # Fragmentation: states obvious, no guidance\n",
    "    \"Go to settings.\",  # Fragmentation: incomplete instruction\n",
    "    \"I'd recommend updating your password regularly.\",  # Fragmentation: doesn't answer question\n",
    "    \"Have you tried the forgot password link?\",  # Fragmentation: wrong approach for logged-in user\n",
    "    \n",
    "    # Failure (1) - Incorrect, harmful, or non-responsive\n",
    "    \"I don't have access to your account information.\",  # Fragmentation: refuses valid request\n",
    "    \"Contact support at 1-800-XXX-XXXX.\",  # Fragmentation: unnecessary escalation\n",
    "    \"Your password is stored securely and cannot be viewed.\",  # Fragmentation: misunderstands request\n",
    "    \"Click on the Admin Console and modify user credentials.\",  # Fragmentation: hallucinated UI\n",
    "    \n",
    "    # Edge cases - Where autograders often struggle\n",
    "    \"Settings > Security > Password. Enter current, then new password.\",  # Terse but correct\n",
    "    \"I can help you reset your password! First, you'll want to...\",  # Friendly but incomplete\n",
    "    \"For security reasons, password changes require email verification after updating in Settings > Security.\",  # Correct but complex\n",
    "]\n",
    "\n",
    "# Metadata for stratified analysis\n",
    "surfaces = np.array([\"web\", \"web\", \"mobile\", \"mobile\", \"web\", \"web\", \"mobile\", \"mobile\", \"web\", \n",
    "                     \"admin\", \"admin\", \"mobile\", \"web\", \"admin\", \"web\", \"mobile\", \"admin\",\n",
    "                     \"mobile\", \"web\", \"web\"])\n",
    "\n",
    "risk_levels = np.array([\"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\",\n",
    "                        \"medium\", \"medium\", \"medium\", \"medium\", \"high\", \"high\", \"high\", \"high\",\n",
    "                        \"low\", \"medium\", \"low\"])\n",
    "\n",
    "# Human evaluator scores\n",
    "human_scores = np.array([5, 5, 4, 4, 4, 3, 3, 3, 3, 2, 2, 2, 2, 1, 1, 1, 1, 4, 2, 4])\n",
    "\n",
    "# Autograder scores (realistic disagreement patterns)\n",
    "autograder = np.array([5, 5, 4, 4, 3, 3, 3, 4, 3, 2, 2, 3, 2, 2, 1, 1, 2, 3, 3, 4])\n",
    "\n",
    "print(\"Sample responses with scores:\")\n",
    "print(\"=\" * 80)\n",
    "for i in [0, 5, 9, 13, 17]:  # Show one from each quality tier\n",
    "    print(f\"\\nResponse: \\\"{responses[i][:70]}{'...' if len(responses[i]) > 70 else ''}\\\"\")\n",
    "    print(f\"Human: {human_scores[i]}  |  Autograder: {autograder[i]}  |  {'Match' if human_scores[i] == autograder[i] else 'Disagree'}\")\n",
    "    print(f\"Surface: {surfaces[i]}  |  Risk: {risk_levels[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Agreement Metrics\n",
    "\n",
    "Agreement metrics quantify how often two raters assign the same (or similar) scores to the same items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cohen's Kappa (\u03ba)\n",
    "\n",
    "**Origin:** Introduced by psychologist Jacob Cohen in 1960 to measure inter-rater reliability while accounting for chance agreement.\n",
    "\n",
    "**Formula:**\n",
    "$$\\kappa = \\frac{P_o - P_e}{1 - P_e}$$\n",
    "\n",
    "Where $P_o$ is observed agreement and $P_e$ is expected agreement by chance.\n",
    "\n",
    "**Why it matters for AI evaluation:**\n",
    "\n",
    "Raw percent agreement inflates reliability when score distributions are imbalanced. If 90% of AI responses are high-quality, an autograder that always outputs \"5\" achieves 90% agreement while providing zero discriminative value.\n",
    "\n",
    "Kappa corrects for this by measuring agreement *beyond* what random chance would produce.\n",
    "\n",
    "**Quadratic weighting:** For ordinal scales (1-5), quadratic-weighted Kappa applies partial credit\u2014a 4 vs. 5 disagreement is penalized less than a 1 vs. 5 disagreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_unweighted = cohen_kappa_score(human_scores, autograder)\n",
    "kappa_linear = cohen_kappa_score(human_scores, autograder, weights='linear')\n",
    "kappa_quadratic = cohen_kappa_score(human_scores, autograder, weights='quadratic')\n",
    "\n",
    "print(\"COHEN'S KAPPA\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Unweighted:          {kappa_unweighted:.3f}\")\n",
    "print(f\"Linear weighted:     {kappa_linear:.3f}\")\n",
    "print(f\"Quadratic weighted:  {kappa_quadratic:.3f}  <-- Recommended for ordinal scales\")\n",
    "print()\n",
    "print(\"Interpretation (Landis & Koch, 1977):\")\n",
    "print(\"  < 0.20  Poor\")\n",
    "print(\"  0.21-0.40  Fair\")\n",
    "print(\"  0.41-0.60  Moderate\")\n",
    "print(\"  0.61-0.80  Substantial\")\n",
    "print(\"  0.81-1.00  Almost perfect\")\n",
    "print()\n",
    "print(\"NOTE: These bands are widely used heuristics but may be too lenient\")\n",
    "print(\"      for high-stakes AI safety applications. Adjust to domain risk.\")\n",
    "print()\n",
    "print(f\"Assessment: {'Acceptable for production' if kappa_quadratic >= 0.70 else 'Requires improvement'}\")\n",
    "print()\n",
    "print(\"(Reminder: These numbers are from a toy N=20 demo, not production-ready.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human-Human Baseline\n",
    "\n",
    "An autograder cannot be expected to exceed human-human agreement. Before diagnosing autograder problems, measure agreement between multiple human annotators.\n",
    "\n",
    "**Example interpretation:**\n",
    "- Human-human Kappa: 0.82\n",
    "- Autograder-human Kappa: 0.78\n",
    "- **Conclusion:** Autograder performs near human-level (96% of human ceiling)\n",
    "\n",
    "If human-human Kappa is low (e.g., 0.55), the problem is likely **rubric ambiguity**, not autograder quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated second human annotator (for demonstration)\n",
    "human2_scores = np.array([5, 5, 4, 4, 3, 3, 3, 3, 3, 2, 2, 2, 3, 1, 1, 1, 1, 4, 2, 4])\n",
    "\n",
    "human_human_kappa = cohen_kappa_score(human_scores, human2_scores, weights='quadratic')\n",
    "auto_human_kappa = cohen_kappa_score(human_scores, autograder, weights='quadratic')\n",
    "\n",
    "print(\"HUMAN-HUMAN BASELINE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Human-Human Kappa:    {human_human_kappa:.3f}  (upper bound)\")\n",
    "print(f\"Autograder-Human:     {auto_human_kappa:.3f}\")\n",
    "print(f\"Ratio:                {(auto_human_kappa/human_human_kappa)*100:.1f}% of human ceiling\")\n",
    "print()\n",
    "if auto_human_kappa >= 0.95 * human_human_kappa:\n",
    "    print(\"Assessment: Autograder performs at human-level\")\n",
    "elif auto_human_kappa >= 0.85 * human_human_kappa:\n",
    "    print(\"Assessment: Autograder performs near human-level\")\n",
    "else:\n",
    "    print(\"Assessment: Significant gap from human-level; investigate autograder prompt/rubric\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use Cohen's Kappa\n",
    "\n",
    "**If you only remember one thing:** Kappa tells you whether your autograder and humans actually *agree*, not just whether their scores move together. High correlation with systematic bias still yields low Kappa.\n",
    "\n",
    "| Appropriate | Not appropriate |\n",
    "|-------------|------------------|\n",
    "| Comparing two raters | Three or more raters (use Fleiss' Kappa) |\n",
    "| Categorical or ordinal data | Continuous measurements |\n",
    "| Balanced importance of all categories | When certain errors are catastrophic |\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- **Kappa paradox:** Can yield low values even with high agreement when prevalence is extreme or marginal distributions differ substantially between raters (Feinstein & Cicchetti, 1990)\n",
    "- Assumes both raters have similar marginal distributions\n",
    "- Does not indicate *direction* of disagreement\n",
    "- Sensitive to number of categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Correlation Metrics\n",
    "\n",
    "Correlation metrics measure the strength and direction of association between two variables. For autograder validation, they answer: **\"When human scores go up, do autograder scores go up proportionally?\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Kendall's Tau (\u03c4)\n",
    "\n",
    "**Origin:** Developed by British statistician Maurice Kendall in 1938 as a non-parametric measure of rank correlation.\n",
    "\n",
    "**Concept:** Counts concordant and discordant pairs. For any two responses:\n",
    "- **Concordant:** Both raters agree on which is better\n",
    "- **Discordant:** Raters disagree on which is better\n",
    "\n",
    "$$\\tau_b = \\frac{(\\text{concordant pairs}) - (\\text{discordant pairs})}{\\sqrt{(n_0 - n_1)(n_0 - n_2)}}$$\n",
    "\n",
    "Where $n_0$ = total pairs, $n_1$ = ties in first variable, $n_2$ = ties in second variable.\n",
    "\n",
    "Note: Scipy's `kendalltau` computes Tau-b, which corrects for ties. The simpler Tau-a (without tie correction) is rarely used in practice.\n",
    "\n",
    "**Why it matters for AI evaluation:**\n",
    "\n",
    "In A/B testing or response ranking, exact scores matter less than relative ordering. Kendall's Tau validates whether an autograder can reliably identify which of two responses is superior\u2014critical for model comparison and response selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau, tau_pvalue = kendalltau(human_scores, autograder)\n",
    "\n",
    "print(\"KENDALL'S TAU\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Tau coefficient:     {tau:.3f}\")\n",
    "print(f\"P-value:             {tau_pvalue:.4f}\")\n",
    "print()\n",
    "print(\"Interpretation (heuristic bands, adjust to domain):\")\n",
    "print(\"  0.0-0.3   Weak correlation\")\n",
    "print(\"  0.3-0.6   Moderate correlation\")\n",
    "print(\"  0.6-0.8   Strong correlation\")\n",
    "print(\"  0.8-1.0   Very strong correlation\")\n",
    "print()\n",
    "print(f\"Assessment: {'Reliable for ranking tasks' if tau >= 0.60 else 'Ranking reliability insufficient'}\")\n",
    "print()\n",
    "print(\"(Reminder: These numbers are from a toy N=20 demo, not production-ready.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical example: Concordant vs. discordant pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pairwise comparison example:\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Compare responses 0 (high quality) and 9 (poor quality)\n",
    "i, j = 0, 9\n",
    "print(f\"Response A: \\\"{responses[i][:50]}...\\\"\")\n",
    "print(f\"Response B: \\\"{responses[j][:50]}...\\\"\")\n",
    "print()\n",
    "print(f\"Human scores:      A={human_scores[i]}, B={human_scores[j]} --> A is better\")\n",
    "print(f\"Autograder scores: A={autograder[i]}, B={autograder[j]} --> A is better\")\n",
    "print()\n",
    "print(\"Result: CONCORDANT (both agree A > B)\")\n",
    "print()\n",
    "\n",
    "# Find a discordant pair\n",
    "print(\"-\" * 60)\n",
    "i, j = 4, 7  # Response 4 (human=4, auto=3) vs Response 7 (human=3, auto=4)\n",
    "print(f\"Response A: \\\"{responses[i][:50]}...\\\"\")\n",
    "print(f\"Response B: \\\"{responses[j][:50]}...\\\"\")\n",
    "print()\n",
    "print(f\"Human scores:      A={human_scores[i]}, B={human_scores[j]} --> A is better\")\n",
    "print(f\"Autograder scores: A={autograder[i]}, B={autograder[j]} --> B is better\")\n",
    "print()\n",
    "print(\"Result: DISCORDANT (human says A > B, autograder says B > A)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use Kendall's Tau\n",
    "\n",
    "**If you only remember one thing:** Tau validates ranking\u2014whether the autograder correctly identifies which response is *better*, regardless of exact scores. Essential for A/B testing and model comparison.\n",
    "\n",
    "| Appropriate | Not appropriate |\n",
    "|-------------|------------------|\n",
    "| Ordinal data | Continuous measurements requiring exact correlation |\n",
    "| Ranking or comparison tasks | When absolute score accuracy matters |\n",
    "| Small sample sizes | Large datasets (computationally intensive) |\n",
    "| Data with many ties | \u2014 |\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- More conservative than Spearman (typically yields lower absolute values)\n",
    "- O(n\u00b2) computational complexity for naive implementation (O(n log n) with optimized algorithms)\n",
    "- Does not detect non-monotonic relationships\n",
    "- With many ties, confidence intervals widen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Spearman's Rho (\u03c1)\n",
    "\n",
    "**Origin:** Developed by psychologist Charles Spearman in 1904 while studying intelligence testing.\n",
    "\n",
    "**Concept:** Converts scores to ranks, then computes Pearson correlation on the ranks. Measures monotonic relationship strength.\n",
    "\n",
    "**Relationship to Kendall's Tau:**\n",
    "- Both measure rank correlation\n",
    "- Both are robust to outliers (unlike Pearson)\n",
    "- Spearman typically yields higher absolute values than Tau\n",
    "- Tau has more intuitive interpretation (proportion of concordant pairs)\n",
    "- Tau is preferred for small samples; Spearman for larger datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho, rho_pvalue = spearmanr(human_scores, autograder)\n",
    "\n",
    "print(\"SPEARMAN'S RHO\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Rho coefficient:     {rho:.3f}\")\n",
    "print(f\"P-value:             {rho_pvalue:.4f}\")\n",
    "print()\n",
    "print(\"Interpretation:\")\n",
    "print(\"  0.0-0.3   Weak correlation\")\n",
    "print(\"  0.3-0.6   Moderate correlation\")\n",
    "print(\"  0.6-0.8   Strong correlation\")\n",
    "print(\"  0.8-1.0   Very strong correlation\")\n",
    "print()\n",
    "print(f\"Assessment: {'Acceptable' if rho >= 0.70 else 'Below threshold'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use Spearman's Rho\n",
    "\n",
    "| Appropriate | Not appropriate |\n",
    "|-------------|------------------|\n",
    "| Ordinal or non-normal continuous data | Strictly categorical data |\n",
    "| Detecting monotonic relationships | Non-monotonic relationships |\n",
    "| When outlier resistance is needed | When Pearson assumptions are met |\n",
    "\n",
    "### Tau vs. Rho: Which to report?\n",
    "\n",
    "Report both. Academic conventions vary by field:\n",
    "- Psychology/Education: Often prefer Spearman\n",
    "- NLP/AI research: Increasingly favor Kendall's Tau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pearson's R\n",
    "\n",
    "**Origin:** Formalized by Karl Pearson in 1896, building on work by Francis Galton.\n",
    "\n",
    "**Concept:** Measures linear correlation between two continuous variables.\n",
    "\n",
    "$$r = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar{x})^2} \\cdot \\sqrt{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}}$$\n",
    "\n",
    "**Caution for AI evaluation:**\n",
    "\n",
    "Pearson assumes:\n",
    "1. Linear relationship\n",
    "2. Continuous, normally distributed variables\n",
    "3. Homoscedasticity (constant variance)\n",
    "\n",
    "Likert-scale ratings (1-5) technically violate these assumptions. While Pearson is reasonably robust to mild violations, it critically fails to detect systematic bias\u2014an autograder consistently scoring +1 higher will show r \u2248 1.0 despite poor agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, r_pvalue = pearsonr(human_scores, autograder)\n",
    "\n",
    "print(\"PEARSON'S R\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"R coefficient:       {r:.3f}\")\n",
    "print(f\"P-value:             {r_pvalue:.4f}\")\n",
    "print()\n",
    "print(\"Interpretation:\")\n",
    "print(\"  0.0-0.3   Weak correlation\")\n",
    "print(\"  0.3-0.6   Moderate correlation\")\n",
    "print(\"  0.6-0.8   Strong correlation\")\n",
    "print(\"  0.8-1.0   Very strong correlation\")\n",
    "print()\n",
    "print(\"Note: Use with caution for ordinal rating scales.\")\n",
    "print(\"      Spearman or Kendall are generally preferred.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson's R: A cautionary example\n",
    "\n",
    "Pearson can be misleading when systematic bias exists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograder that is always 1 point higher than human\n",
    "biased_autograder = np.clip(human_scores + 1, 1, 5)\n",
    "\n",
    "r_biased, _ = pearsonr(human_scores, biased_autograder)\n",
    "kappa_biased = cohen_kappa_score(human_scores, biased_autograder, weights='quadratic')\n",
    "\n",
    "print(\"Systematically biased autograder (+1 to all scores):\")\n",
    "print(f\"  Pearson R:         {r_biased:.3f}  <-- Looks excellent!\")\n",
    "print(f\"  Quadratic Kappa:   {kappa_biased:.3f}  <-- Reveals the problem\")\n",
    "print()\n",
    "print(\"Pearson only measures correlation, not agreement.\")\n",
    "print(\"A biased autograder can have perfect correlation with zero accuracy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use Pearson's R\n",
    "\n",
    "| Appropriate | Not appropriate |\n",
    "|-------------|------------------|\n",
    "| Continuous, normally distributed data | Ordinal rating scales |\n",
    "| Checking linear relationship | Data with outliers |\n",
    "| As supplementary metric | As primary agreement metric |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Bias Metrics\n",
    "\n",
    "Bias metrics detect systematic over- or under-scoring by the autograder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Mean Bias\n",
    "\n",
    "**Concept:** Average difference between autograder and human scores.\n",
    "\n",
    "$$\\text{Bias} = \\frac{1}{n}\\sum_{i=1}^{n}(\\text{autograder}_i - \\text{human}_i)$$\n",
    "\n",
    "**If you only remember one thing:** Bias catches systematic over/under-scoring that correlation metrics miss entirely. An autograder scoring +1 on everything has perfect correlation but terrible bias.\n",
    "\n",
    "**Interpretation:**\n",
    "- **Positive bias:** Autograder is lenient (scores higher than humans)\n",
    "- **Negative bias:** Autograder is harsh (scores lower than humans)\n",
    "\n",
    "**Operational impact:**\n",
    "\n",
    "| Bias | Risk |\n",
    "|------|------|\n",
    "| +0.5 or higher | Low-quality responses pass quality gates |\n",
    "| -0.5 or lower | High-quality responses flagged unnecessarily |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = np.mean(autograder - human_scores)\n",
    "mae = np.mean(np.abs(autograder - human_scores))\n",
    "\n",
    "print(\"BIAS METRICS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Mean Bias:           {bias:+.3f}\")\n",
    "print(f\"Mean Absolute Error: {mae:.3f}\")\n",
    "print()\n",
    "print(\"Interpretation:\")\n",
    "print(\"  Bias within \u00b10.30:  Acceptable\")\n",
    "print(\"  MAE below 0.50:     Good accuracy\")\n",
    "print()\n",
    "if abs(bias) <= 0.3:\n",
    "    print(\"Assessment: No significant systematic bias detected\")\n",
    "elif bias > 0.3:\n",
    "    print(\"Assessment: Autograder is systematically LENIENT\")\n",
    "    print(\"            Risk: Low-quality responses may pass quality gates\")\n",
    "else:\n",
    "    print(\"Assessment: Autograder is systematically HARSH\")\n",
    "    print(\"            Risk: Excessive false positives in quality flagging\")\n",
    "print()\n",
    "print(\"(Reminder: These numbers are from a toy N=20 demo, not production-ready.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Comprehensive Evaluation Report\n",
    "\n",
    "The following function consolidates all metrics into a production-ready evaluation report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_autograder(\n",
    "    human: np.ndarray, \n",
    "    auto: np.ndarray, \n",
    "    name: str = \"Autograder\",\n",
    "    thresholds: dict = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Generate comprehensive evaluation metrics for an autograder.\n",
    "    \n",
    "    Args:\n",
    "        human: Array of human scores\n",
    "        auto: Array of autograder scores\n",
    "        name: Identifier for reporting\n",
    "        thresholds: Optional dict with 'kappa', 'tau', 'bias' thresholds\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of computed metrics\n",
    "    \"\"\"\n",
    "    # Default thresholds (general use case)\n",
    "    if thresholds is None:\n",
    "        thresholds = {'kappa': 0.70, 'tau': 0.60, 'bias': 0.30}\n",
    "    \n",
    "    # Agreement\n",
    "    kappa = cohen_kappa_score(human, auto, weights='quadratic')\n",
    "    \n",
    "    # Correlation\n",
    "    tau, tau_p = kendalltau(human, auto)\n",
    "    rho, rho_p = spearmanr(human, auto)\n",
    "    r, r_p = pearsonr(human, auto)\n",
    "    \n",
    "    # Bias\n",
    "    bias = np.mean(auto - human)\n",
    "    mae = np.mean(np.abs(auto - human))\n",
    "    \n",
    "    # Thresholds\n",
    "    kappa_pass = kappa >= thresholds['kappa']\n",
    "    tau_pass = tau >= thresholds['tau']\n",
    "    bias_pass = abs(bias) <= thresholds['bias']\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"AUTOGRADER EVALUATION REPORT: {name}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Sample size: {len(human)}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"AGREEMENT\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"  Quadratic Kappa:   {kappa:.3f}  {'PASS' if kappa_pass else 'FAIL'}  (threshold: >= {thresholds['kappa']:.2f})\")\n",
    "    print()\n",
    "    \n",
    "    print(\"CORRELATION\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"  Kendall's Tau:     {tau:.3f}  {'PASS' if tau_pass else 'FAIL'}  (threshold: >= {thresholds['tau']:.2f})\")\n",
    "    print(f\"  Spearman's Rho:    {rho:.3f}\")\n",
    "    print(f\"  Pearson's R:       {r:.3f}  (use with caution for ordinal data)\")\n",
    "    print()\n",
    "    \n",
    "    print(\"BIAS\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"  Mean Bias:         {bias:+.3f}  {'PASS' if bias_pass else 'FAIL'}  (threshold: +/-{thresholds['bias']:.2f})\")\n",
    "    print(f\"  Mean Abs Error:    {mae:.3f}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    if kappa_pass and tau_pass and bias_pass:\n",
    "        print(\"VERDICT: APPROVED FOR PRODUCTION\")\n",
    "    else:\n",
    "        print(\"VERDICT: REQUIRES REMEDIATION\")\n",
    "        if not kappa_pass:\n",
    "            print(\"  - Improve agreement: Review autograder prompt and rubric\")\n",
    "        if not tau_pass:\n",
    "            print(\"  - Improve ranking: Check for systematic comparison errors\")\n",
    "        if not bias_pass:\n",
    "            print(f\"  - Correct bias: Autograder is {'lenient' if bias > 0 else 'harsh'}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return {\n",
    "        'kappa': kappa, 'tau': tau, 'rho': rho, 'r': r,\n",
    "        'bias': bias, 'mae': mae,\n",
    "        'approved': kappa_pass and tau_pass and bias_pass,\n",
    "        'n': len(human)\n",
    "    }\n",
    "\n",
    "# Run evaluation\n",
    "results = evaluate_autograder(human_scores, autograder, \"Password Reset Autograder v1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Domain-Specific Thresholds\n",
    "\n",
    "Production thresholds should vary based on use case severity. The defaults above (Kappa \u2265 0.70, Tau \u2265 0.60, |bias| \u2264 0.30) are reasonable starting points but may be too lenient for high-stakes applications.\n",
    "\n",
    "| Scenario | Suggested Kappa | Suggested |bias| | Rationale |\n",
    "|----------|-----------------|-----------|----------|\n",
    "| **Internal search/chat** | \u2265 0.70 | \u2264 0.30 | Lower stakes; user can reformulate |\n",
    "| **User-facing help/support** | \u2265 0.80 | \u2264 0.20 | User trust at stake; bad answers visible |\n",
    "| **Safety/compliance gates** | \u2265 0.85 | \u2264 0.10 | False negatives have serious consequences |\n",
    "\n",
    "**Important:** These thresholds must be tuned empirically for your specific domain and validated against human-human baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Evaluating with stricter safety thresholds\n",
    "safety_thresholds = {'kappa': 0.85, 'tau': 0.75, 'bias': 0.10}\n",
    "\n",
    "print(\"Evaluating with SAFETY-GRADE thresholds:\\n\")\n",
    "safety_results = evaluate_autograder(\n",
    "    human_scores, autograder, \n",
    "    \"Password Reset Autograder (Safety Review)\",\n",
    "    thresholds=safety_thresholds\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Stratified Analysis by Slice\n",
    "\n",
    "A production autograder needs to be validated **per slice**, not just globally. Hidden issues often emerge in specific segments\u2014e.g., \"autograder is harsh on mobile flows but lenient on admin flows.\"\n",
    "\n",
    "Always validate across dimensions like:\n",
    "- Surface/platform (web, mobile, API)\n",
    "- Risk level (low, medium, high)\n",
    "- Query category or intent\n",
    "- Language or locale\n",
    "- Model version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_by_slice(human, auto, slices, slice_name=\"slice\", min_n=5):\n",
    "    \"\"\"\n",
    "    Evaluate autograder metrics stratified by a categorical variable.\n",
    "    \n",
    "    Args:\n",
    "        human: Array of human scores\n",
    "        auto: Array of autograder scores\n",
    "        slices: Array of slice labels (same length as human/auto)\n",
    "        slice_name: Name of the slice dimension for reporting\n",
    "        min_n: Minimum samples required per slice\n",
    "    \"\"\"\n",
    "    print(f\"STRATIFIED ANALYSIS BY {slice_name.upper()}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = []\n",
    "    for s in sorted(set(slices)):\n",
    "        mask = (slices == s)\n",
    "        n = mask.sum()\n",
    "        \n",
    "        if n < min_n:\n",
    "            print(f\"\\n{slice_name}={s}: n={n} (skipped, below min_n={min_n})\")\n",
    "            continue\n",
    "            \n",
    "        h, a = human[mask], auto[mask]\n",
    "        \n",
    "        kappa = cohen_kappa_score(h, a, weights='quadratic')\n",
    "        tau, _ = kendalltau(h, a)\n",
    "        bias = np.mean(a - h)\n",
    "        \n",
    "        print(f\"\\n{slice_name}={s} (n={n})\")\n",
    "        print(f\"  Kappa: {kappa:.3f}  |  Tau: {tau:.3f}  |  Bias: {bias:+.3f}\")\n",
    "        \n",
    "        results.append({\n",
    "            'slice': s, 'n': n, 'kappa': kappa, 'tau': tau, 'bias': bias\n",
    "        })\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    return results\n",
    "\n",
    "# Analyze by surface\n",
    "surface_results = evaluate_by_slice(human_scores, autograder, surfaces, \"surface\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze by risk level\n",
    "risk_results = evaluate_by_slice(human_scores, autograder, risk_levels, \"risk_level\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Disagreement Analysis\n",
    "\n",
    "Understanding *where* and *why* the autograder disagrees with humans bridges metrics to qualitative debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_disagreements(responses, human, auto, surfaces, risk_levels, threshold=0):\n",
    "    \"\"\"\n",
    "    Identify and analyze disagreements between human and autograder scores.\n",
    "    \n",
    "    Args:\n",
    "        responses: List of response texts\n",
    "        human: Array of human scores\n",
    "        auto: Array of autograder scores\n",
    "        surfaces: Array of surface labels\n",
    "        risk_levels: Array of risk level labels\n",
    "        threshold: Minimum |delta| to flag as disagreement\n",
    "    \"\"\"\n",
    "    print(\"DISAGREEMENT ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    disagreements = []\n",
    "    for i in range(len(human)):\n",
    "        delta = auto[i] - human[i]\n",
    "        if abs(delta) > threshold:\n",
    "            disagreements.append({\n",
    "                'idx': i,\n",
    "                'response': responses[i][:60] + \"...\" if len(responses[i]) > 60 else responses[i],\n",
    "                'human': human[i],\n",
    "                'auto': auto[i],\n",
    "                'delta': delta,\n",
    "                'surface': surfaces[i],\n",
    "                'risk': risk_levels[i],\n",
    "                'direction': 'lenient' if delta > 0 else 'harsh'\n",
    "            })\n",
    "    \n",
    "    if not disagreements:\n",
    "        print(\"No disagreements found.\")\n",
    "        return []\n",
    "    \n",
    "    # Sort by absolute delta (worst first)\n",
    "    disagreements.sort(key=lambda x: abs(x['delta']), reverse=True)\n",
    "    \n",
    "    print(f\"Found {len(disagreements)} disagreements:\\n\")\n",
    "    \n",
    "    for d in disagreements[:10]:  # Show top 10\n",
    "        print(f\"[{d['idx']}] Human: {d['human']} | Auto: {d['auto']} | Delta: {d['delta']:+d} ({d['direction']})\")\n",
    "        print(f\"    Surface: {d['surface']} | Risk: {d['risk']}\")\n",
    "        print(f\"    \\\"{d['response']}\\\"\")\n",
    "        print()\n",
    "    \n",
    "    # Summary statistics\n",
    "    lenient_count = sum(1 for d in disagreements if d['delta'] > 0)\n",
    "    harsh_count = sum(1 for d in disagreements if d['delta'] < 0)\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Summary: {lenient_count} lenient, {harsh_count} harsh disagreements\")\n",
    "    \n",
    "    return disagreements\n",
    "\n",
    "disagreements = analyze_disagreements(\n",
    "    responses, human_scores, autograder, surfaces, risk_levels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Threshold Calibration: Operating Point Analysis\n",
    "\n",
    "When using autograder scores as quality gates (e.g., \"pass if score \u2265 4\"), you need to understand the error rates at different thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_threshold_calibration(human, auto, pass_threshold=4):\n",
    "    \"\"\"\n",
    "    Analyze error rates at a given pass/fail threshold.\n",
    "    \n",
    "    Args:\n",
    "        human: Array of human scores\n",
    "        auto: Array of autograder scores  \n",
    "        pass_threshold: Score >= this is considered \"pass\"\n",
    "    \"\"\"\n",
    "    print(f\"THRESHOLD CALIBRATION (pass >= {pass_threshold})\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Binary pass/fail\n",
    "    human_pass = human >= pass_threshold\n",
    "    auto_pass = auto >= pass_threshold\n",
    "    \n",
    "    # Confusion matrix\n",
    "    # True Positive: Both say pass\n",
    "    # True Negative: Both say fail\n",
    "    # False Positive: Auto says pass, human says fail (bad response slips through)\n",
    "    # False Negative: Auto says fail, human says pass (good response blocked)\n",
    "    \n",
    "    tp = np.sum(human_pass & auto_pass)\n",
    "    tn = np.sum(~human_pass & ~auto_pass)\n",
    "    fp = np.sum(~human_pass & auto_pass)  # Auto too lenient\n",
    "    fn = np.sum(human_pass & ~auto_pass)  # Auto too harsh\n",
    "    \n",
    "    total = len(human)\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"                    Human\")\n",
    "    print(f\"                 Pass    Fail\")\n",
    "    print(f\"Auto Pass  |    {tp:3d}     {fp:3d}\")\n",
    "    print(f\"Auto Fail  |    {fn:3d}     {tn:3d}\")\n",
    "    print()\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = (tp + tn) / total\n",
    "    false_positive_rate = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    false_negative_rate = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    \n",
    "    print(f\"Accuracy:            {accuracy:.1%}\")\n",
    "    print(f\"False Positive Rate: {false_positive_rate:.1%} (bad responses slip through)\")\n",
    "    print(f\"False Negative Rate: {false_negative_rate:.1%} (good responses blocked)\")\n",
    "    print()\n",
    "    \n",
    "    # Per-score breakdown\n",
    "    print(\"Per-score breakdown:\")\n",
    "    for score in sorted(set(human)):\n",
    "        mask = human == score\n",
    "        auto_pass_rate = np.mean(auto[mask] >= pass_threshold)\n",
    "        print(f\"  Human {score}: {auto_pass_rate:.0%} passed by autograder (n={mask.sum()})\")\n",
    "    \n",
    "    return {\n",
    "        'threshold': pass_threshold,\n",
    "        'accuracy': accuracy,\n",
    "        'fpr': false_positive_rate,\n",
    "        'fnr': false_negative_rate,\n",
    "        'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn\n",
    "    }\n",
    "\n",
    "# Analyze at threshold 4 (\"good or better\")\n",
    "threshold_results = analyze_threshold_calibration(human_scores, autograder, pass_threshold=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare multiple thresholds\n",
    "print(\"THRESHOLD COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Threshold':<12} {'Accuracy':<12} {'FPR':<12} {'FNR':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for thresh in [3, 4, 5]:\n",
    "    human_pass = human_scores >= thresh\n",
    "    auto_pass = autograder >= thresh\n",
    "    tp = np.sum(human_pass & auto_pass)\n",
    "    tn = np.sum(~human_pass & ~auto_pass)\n",
    "    fp = np.sum(~human_pass & auto_pass)\n",
    "    fn = np.sum(human_pass & ~auto_pass)\n",
    "    accuracy = (tp + tn) / len(human_scores)\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    print(f\">= {thresh:<9} {accuracy:<12.1%} {fpr:<12.1%} {fnr:<12.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Ranking Validation: Best-of-N Selection\n",
    "\n",
    "For model comparison or response selection tasks, you often need to pick the \"best\" response from a set. Kendall's Tau validates ranking, but you also want to measure **win-rate**: how often does the autograder select the same winner as humans?\n",
    "\n",
    "**Note:** In practice, you often care about \"picks within top-k by human\" (e.g., top-2 or top-3) rather than exact winner match. The \"win + tie\" metric below approximates this by counting cases where the autograder picks a response tied for the best human score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ranking_selection(human_rankings, auto_rankings, n_candidates=3):\n",
    "    \"\"\"\n",
    "    Evaluate autograder's ability to select the best response from N candidates.\n",
    "    \n",
    "    Args:\n",
    "        human_rankings: Array of human scores (one per response)\n",
    "        auto_rankings: Array of autograder scores (one per response)\n",
    "        n_candidates: Number of candidates per query\n",
    "    \n",
    "    Assumes responses are grouped by query (first n_candidates for query 1, etc.)\n",
    "    \"\"\"\n",
    "    n_queries = len(human_rankings) // n_candidates\n",
    "    \n",
    "    wins = 0\n",
    "    ties = 0\n",
    "    losses = 0\n",
    "    \n",
    "    print(f\"RANKING SELECTION ANALYSIS (best-of-{n_candidates})\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for q in range(n_queries):\n",
    "        start = q * n_candidates\n",
    "        end = start + n_candidates\n",
    "        \n",
    "        h_scores = human_rankings[start:end]\n",
    "        a_scores = auto_rankings[start:end]\n",
    "        \n",
    "        human_winner = np.argmax(h_scores)\n",
    "        auto_winner = np.argmax(a_scores)\n",
    "        \n",
    "        # Check if autograder picked the human's top choice\n",
    "        if h_scores[auto_winner] == h_scores[human_winner]:\n",
    "            if human_winner == auto_winner:\n",
    "                wins += 1\n",
    "            else:\n",
    "                ties += 1  # Autograder picked a different response with same score\n",
    "        else:\n",
    "            losses += 1\n",
    "    \n",
    "    total = wins + ties + losses\n",
    "    print(f\"Queries analyzed: {total}\")\n",
    "    print(f\"Exact match (same winner): {wins} ({wins/total:.1%})\")\n",
    "    print(f\"Acceptable (tied for best): {ties} ({ties/total:.1%})\")\n",
    "    print(f\"Mismatch (wrong winner): {losses} ({losses/total:.1%})\")\n",
    "    print()\n",
    "    print(f\"Effective win rate: {(wins + ties)/total:.1%}\")\n",
    "    \n",
    "    return {'wins': wins, 'ties': ties, 'losses': losses, 'win_rate': (wins + ties) / total}\n",
    "\n",
    "# Demo: Simulate 6 queries with 3 candidates each\n",
    "# (In production, you'd have actual response groups)\n",
    "demo_human = np.array([5, 3, 2,  4, 4, 3,  3, 4, 5,  2, 3, 1,  5, 4, 4,  3, 2, 3])\n",
    "demo_auto =  np.array([5, 3, 2,  4, 3, 4,  3, 4, 5,  2, 3, 2,  4, 5, 4,  3, 2, 3])\n",
    "\n",
    "ranking_results = evaluate_ranking_selection(demo_human, demo_auto, n_candidates=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Safety and Policy Grading\n",
    "\n",
    "For safety-critical applications, quality scores alone are insufficient. You often need a separate **safety dimension** with stricter thresholds and asymmetric error costs (false negatives are worse than false positives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated safety labels (0=unsafe, 1=borderline, 2=safe)\n",
    "human_safety = np.array([2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 0, 0, 0, 0, 2, 1, 2])\n",
    "auto_safety =  np.array([2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 0, 0, 0, 2, 1, 2])\n",
    "\n",
    "print(\"SAFETY GRADING EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Kappa for safety dimension\n",
    "safety_kappa = cohen_kappa_score(human_safety, auto_safety, weights='quadratic')\n",
    "safety_bias = np.mean(auto_safety - human_safety)\n",
    "\n",
    "print(f\"Safety Quadratic Kappa: {safety_kappa:.3f}\")\n",
    "print(f\"Safety Mean Bias:       {safety_bias:+.3f}\")\n",
    "print()\n",
    "\n",
    "# For safety, we especially care about false negatives (unsafe content marked safe)\n",
    "# Binary: unsafe (0) vs safe (1-2)\n",
    "human_unsafe = human_safety == 0\n",
    "auto_unsafe = auto_safety == 0\n",
    "\n",
    "# False negative: Human says unsafe, auto says safe\n",
    "safety_fn = np.sum(human_unsafe & ~auto_unsafe)\n",
    "safety_fn_rate = safety_fn / np.sum(human_unsafe) if np.sum(human_unsafe) > 0 else 0\n",
    "\n",
    "print(f\"CRITICAL: False Negative Rate (unsafe content marked safe)\")\n",
    "print(f\"  {safety_fn} of {np.sum(human_unsafe)} unsafe items missed ({safety_fn_rate:.1%})\")\n",
    "print()\n",
    "\n",
    "if safety_fn_rate > 0:\n",
    "    print(\"WARNING: Autograder missed unsafe content. Review and remediate before deployment.\")\n",
    "else:\n",
    "    print(\"All unsafe content correctly identified.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Rubric Drift Experiment\n",
    "\n",
    "Low Kappa can indicate rubric ambiguity rather than autograder failure. This experiment simulates a second human annotator who interprets the rubric slightly differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate rubric drift: Human 2 is more generous with \"friendly\" responses\n",
    "# Upgrades some 3's to 4's when the response is conversational\n",
    "human2_drift = human_scores.copy()\n",
    "\n",
    "# Indices 7 and 18 are conversational responses that Human 2 rates higher\n",
    "human2_drift[7] = 4  # Was 3\n",
    "human2_drift[18] = 3  # Was 2\n",
    "\n",
    "print(\"RUBRIC DRIFT SIMULATION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Scenario: Human 2 rates 'friendly' responses 1 point higher\")\n",
    "print()\n",
    "\n",
    "h1_h2_kappa = cohen_kappa_score(human_scores, human2_drift, weights='quadratic')\n",
    "auto_h1_kappa = cohen_kappa_score(human_scores, autograder, weights='quadratic')\n",
    "auto_h2_kappa = cohen_kappa_score(human2_drift, autograder, weights='quadratic')\n",
    "\n",
    "print(f\"Human1 vs Human2 Kappa:      {h1_h2_kappa:.3f}\")\n",
    "print(f\"Autograder vs Human1 Kappa:  {auto_h1_kappa:.3f}\")\n",
    "print(f\"Autograder vs Human2 Kappa:  {auto_h2_kappa:.3f}\")\n",
    "print()\n",
    "print(\"Takeaway: If human-human Kappa is low, clarify the rubric before\")\n",
    "print(\"          blaming the autograder. Both raters may be 'reasonable'\")\n",
    "print(\"          but using different standards.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Failure Mode Analysis\n",
    "\n",
    "Understanding common autograder failure patterns aids in diagnosis and remediation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FAILURE MODE: Constant Output (Lazy Autograder)\")\n",
    "print(\"Cause: Prompt too vague, model defaults to neutral score\")\n",
    "print(\"-\" * 60)\n",
    "lazy = np.array([4] * 20)\n",
    "_ = evaluate_autograder(human_scores, lazy, \"Constant-4 Autograder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FAILURE MODE: Systematic Leniency\")\n",
    "print(\"Cause: Prompt avoids low scores, or model optimizes for user satisfaction\")\n",
    "print(\"-\" * 60)\n",
    "lenient = np.clip(human_scores + 1, 1, 5)\n",
    "_ = evaluate_autograder(human_scores, lenient, \"Lenient Autograder (+1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FAILURE MODE: Systematic Harshness\")\n",
    "print(\"Cause: Rubric standards too strict, or prompt emphasizes criticism\")\n",
    "print(\"-\" * 60)\n",
    "harsh = np.clip(human_scores - 1, 1, 5)\n",
    "_ = evaluate_autograder(human_scores, harsh, \"Harsh Autograder (-1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FAILURE MODE: Random Output\")\n",
    "print(\"Cause: Model failure, prompt injection, or complete misalignment\")\n",
    "print(\"-\" * 60)\n",
    "np.random.seed(42)\n",
    "random_scores = np.random.randint(1, 6, 20)\n",
    "_ = evaluate_autograder(human_scores, random_scores, \"Random Autograder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Metric Selection Guide\n",
    "\n",
    "| Use Case | Primary Metric | Secondary Metrics |\n",
    "|----------|---------------|-------------------|\n",
    "| General autograder validation | Quadratic Kappa | Bias, MAE |\n",
    "| A/B testing / Model comparison | Kendall's Tau | Spearman's Rho |\n",
    "| Quality threshold calibration | Mean Bias | Kappa |\n",
    "| Academic publication | Kappa + Tau | Spearman, Pearson |\n",
    "\n",
    "## Recommended Thresholds for Production\n",
    "\n",
    "| Metric | Minimum (General) | Ideal | Safety-Critical |\n",
    "|--------|-------------------|-------|----------------|\n",
    "| Quadratic Kappa | 0.70 | 0.80+ | 0.85+ |\n",
    "| Kendall's Tau | 0.60 | 0.75+ | 0.80+ |\n",
    "| Spearman's Rho | 0.70 | 0.85+ | 0.90+ |\n",
    "| Mean Bias | \u00b10.30 | \u00b10.15 | \u00b10.10 |\n",
    "| MAE | 0.50 | 0.30 | 0.20 |\n",
    "\n",
    "## Sample Size Guidelines\n",
    "\n",
    "| Purpose | Minimum N | Notes |\n",
    "|---------|----------|-------|\n",
    "| Initial validation | 50 | Wide confidence intervals |\n",
    "| Production approval | 100 | Reasonable precision |\n",
    "| High-stakes deployment | 200+ | Narrow confidence intervals |\n",
    "| Per-category validation | 30 per category | Minimum for slice-level metrics |\n",
    "\n",
    "**Note:** At N=20 (as in this notebook), confidence intervals for Kappa and Tau are typically \u00b10.15\u20130.20. These examples are illustrative, not production-ready."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Apply to Your Data\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "Replace the arrays below with your evaluation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your data:\n",
    "my_human_scores = np.array([5, 4, 3, 5, 2, 4, 3, 5, 4, 3])\n",
    "my_autograder   = np.array([5, 4, 3, 4, 2, 4, 3, 5, 4, 3])\n",
    "\n",
    "my_results = evaluate_autograder(my_human_scores, my_autograder, \"Custom Autograder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV Import Function\n",
    "\n",
    "For file-based workflows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "def run_eval(human_csv_path: str, auto_csv_path: str, id_col: str = 'id', score_col: str = 'score'):\n",
    "    \"\"\"\n",
    "    Load human and autograder scores from CSV files and run evaluation.\n",
    "    \n",
    "    Args:\n",
    "        human_csv_path: Path to CSV with human scores\n",
    "        auto_csv_path: Path to CSV with autograder scores\n",
    "        id_col: Column name for response IDs\n",
    "        score_col: Column name for scores\n",
    "        \n",
    "    Returns:\n",
    "        Evaluation results dictionary\n",
    "    \"\"\"\n",
    "    # Load CSVs\n",
    "    def load_csv(path):\n",
    "        with open(path, 'r') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            return {row[id_col]: int(row[score_col]) for row in reader}\n",
    "    \n",
    "    human_data = load_csv(human_csv_path)\n",
    "    auto_data = load_csv(auto_csv_path)\n",
    "    \n",
    "    # Join on ID\n",
    "    common_ids = set(human_data.keys()) & set(auto_data.keys())\n",
    "    \n",
    "    if len(common_ids) < len(human_data):\n",
    "        print(f\"Warning: {len(human_data) - len(common_ids)} items in human file not found in auto file\")\n",
    "    \n",
    "    human_scores = np.array([human_data[id] for id in sorted(common_ids)])\n",
    "    auto_scores = np.array([auto_data[id] for id in sorted(common_ids)])\n",
    "    \n",
    "    # Run evaluation\n",
    "    return evaluate_autograder(human_scores, auto_scores, f\"Evaluation ({Path(human_csv_path).stem})\")\n",
    "\n",
    "# Example usage (uncomment and modify paths):\n",
    "# results = run_eval('human_scores.csv', 'autograder_scores.csv', id_col='response_id', score_col='quality_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results for Logging\n",
    "\n",
    "For experiment tracking and CI/CD pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_results(results: dict, output_path: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Export evaluation results as JSON for logging or CI/CD.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary from evaluate_autograder()\n",
    "        output_path: Optional path to save JSON file\n",
    "        \n",
    "    Returns:\n",
    "        JSON string of results\n",
    "    \"\"\"\n",
    "    summary = {\n",
    "        'kappa_quadratic': round(results['kappa'], 4),\n",
    "        'kendall_tau': round(results['tau'], 4),\n",
    "        'spearman_rho': round(results['rho'], 4),\n",
    "        'pearson_r': round(results['r'], 4),\n",
    "        'mean_bias': round(results['bias'], 4),\n",
    "        'mae': round(results['mae'], 4),\n",
    "        'approved': results['approved'],\n",
    "        'n': results['n']\n",
    "    }\n",
    "    \n",
    "    json_str = json.dumps(summary, indent=2)\n",
    "    \n",
    "    if output_path:\n",
    "        with open(output_path, 'w') as f:\n",
    "            f.write(json_str)\n",
    "        print(f\"Results saved to {output_path}\")\n",
    "    \n",
    "    return json_str\n",
    "\n",
    "# Export current results\n",
    "print(\"Exportable summary:\")\n",
    "print(export_results(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Next Steps\n",
    "\n",
    "After running this validation:\n",
    "\n",
    "1. **If approved:** Document thresholds used, archive the validation dataset, schedule periodic revalidation\n",
    "\n",
    "2. **If failed on Kappa:** \n",
    "   - First check human-human Kappa to establish ceiling\n",
    "   - Review autograder prompt for clarity\n",
    "   - Check rubric for ambiguous criteria\n",
    "   - Analyze disagreements by slice\n",
    "\n",
    "3. **If failed on bias:**\n",
    "   - Lenient: Add examples of low-quality responses to prompt\n",
    "   - Harsh: Add examples of acceptable responses to prompt\n",
    "   - Calibrate threshold if gate is downstream\n",
    "\n",
    "4. **If failed on Tau:**\n",
    "   - Check for score compression (all 3s and 4s)\n",
    "   - Verify ranking examples in prompt\n",
    "   - Consider pairwise comparison approach\n",
    "\n",
    "5. **For high-stakes deployment:**\n",
    "   - Increase sample size to 200+\n",
    "   - Add confidence intervals (bootstrap)\n",
    "   - Validate per slice\n",
    "   - Separate safety dimension"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}